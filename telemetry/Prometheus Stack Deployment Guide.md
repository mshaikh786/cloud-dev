# Prometheus Stack Deployment Guide

## Overview

This document provides comprehensive instructions for deploying the Prometheus Monitoring Stack (kube-prometheus-stack) on Kubernetes using Helm. The stack includes Prometheus, Alertmanager, Grafana, Node Exporter, and Kube State Metrics components for a complete monitoring solution.

## Prerequisites

- Kubernetes cluster (v1.19+)
- Helm v3 installed
- kubectl command-line tool configured
- Administrative access to the cluster

## Architecture

The kube-prometheus-stack deployment includes:

- **Prometheus Server**: Time-series database for metrics collection and storage
- **Alertmanager**: Handles alerts from Prometheus
- **Grafana**: Visualization and dashboarding
- **Node Exporter**: Collects hardware and OS metrics from nodes
- **Kube State Metrics**: Generates metrics about Kubernetes objects
- **Prometheus Operator**: Manages Prometheus instances and configuration

```
┌─────────────────────────────────────────────────────────────┐
│                  kube-prometheus-stack                      │
├───────────┬───────────┬───────────┬───────────┬─────────────┤
│ Prometheus│Alertmanager│  Grafana  │    Node   │  Kube State│
│  Server   │           │           │  Exporter │  Metrics    │
├───────────┴───────────┴───────────┴───────────┴─────────────┤
│                    Prometheus Operator                      │
└─────────────────────────────────────────────────────────────┘
```

## Deployment Steps

### 1. Add the Prometheus Community Helm Repository

```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
```

### 2. Examine and Customize Values

```bash
# Download default values for customization
helm inspect values prometheus-community/kube-prometheus-stack > kube-prometheus-stack.values
```

### 3. Configure Custom Values

Edit the `kube-prometheus-stack.values` file to customize your deployment. The following sections highlight key configurations:

#### a. Service Monitor Configuration

Modify the `prometheusSpec.serviceMonitorSelectorNilUsesHelmValues` settings to `false` to allow Prometheus to discover ServiceMonitors across all namespaces:

```yaml
## Configuration for Prometheus
##
prometheus:
  prometheusSpec:
    # If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
    # prometheus resource to be created with selectors based on values in the helm deployment,
    # which will also match the servicemonitors created
    serviceMonitorSelectorNilUsesHelmValues: false
```

#### b. Additional Scrape Configurations

Add custom scrape configurations for special metric sources. The example below adds GPU metrics collection:

```yaml
    # AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
    # are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
    # as specified in the official Prometheus documentation:
    # https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config.
    additionalScrapeConfigs:
    - job_name: gpu-metrics
      scrape_interval: 1s
      metrics_path: /metrics
      scheme: http
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - gpu-operator
      relabel_configs:
      - source_labels: [__meta_kubernetes_endpoints_name]
        action: drop
        regex: .*-node-feature-discovery-master
      - source_labels: [__meta_kubernetes_pod_node_name]
        action: replace
        target_label: kubernetes_node
```

## Resource Calculation and Provisioning for Prometheus

### Overview

Proper resource allocation is critical for Prometheus performance, especially when monitoring GPU metrics across a large cluster. This section outlines how to calculate and provision appropriate resources for your Prometheus deployment.

### Resource Calculation Guidelines

#### Memory Requirements

Memory is typically the most critical resource for Prometheus. Use these guidelines:

* **Base Rule**: Prometheus typically requires approximately 3 kilobytes (kB) of memory per time series.
* **For GPU Monitoring**: Allocate additional memory for DCGM-exporter metrics which are numerous and frequently updated.
* **Query Overhead**: Add 2-4GB extra memory to handle complex queries and dashboards.

#### Storage Requirements

Storage needs can be calculated using:

needed_disk_space = retention_time_seconds * ingested_samples_per_second * bytes_per_sample

* **Retention Period**: 6 days = 518,400 seconds
* **Bytes per Sample**: Typically 1-2 bytes per sample for compressed storage
* **Buffer**: Add 50% additional storage as a safety margin

#### CPU Requirements

The Prometheus agent requires about 1 CPU core and 1 GB memory to collect 1 million metrics.

* **Base CPU**: 0.5-1 core for basic operations
* **Ingestion CPU**: Scale based on samples per second (typically 1 core per 200K-300K samples/second)
* **Query CPU**: Additional 1-4 cores depending on query complexity and frequency

### Simple Rules of Thumb

1. **Memory Rule**: Total Memory (GB) = (Number of time series × 3KB) ÷ 1,000,000 + 2GB for query overhead
   
   * Example: 100,000 series × 3KB = 300MB + 2GB = ~2.3GB needed

2. **CPU Rule**: Start with 2 CPU cores for up to 300K samples/second, then add 1 core for each additional 200K samples/second

3. **Storage Rule**: Daily storage needs (GB) = Samples per second × 86,400 × 2 bytes × 1.5 (buffer) ÷ 1,000,000,000
   
   * Example: 10,000 samples/second × 86,400 × 2 bytes × 1.5 ÷ 1,000,000,000 = ~2.6GB per day

4. **Full Retention Storage**: Multiply daily storage by retention period (6 days)

### Configuration Example

Here's an example configuration for a mid-sized cluster with approximately 100K time series:

```yaml
prometheus:
  prometheusSpec:
    resources:
      requests:
        memory: "4Gi"
        cpu: "2000m"
      limits:
        memory: "8Gi"
        cpu: "4000m"

    retention: 6d
    retentionSize: "80GB"

    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: standard # Use your cluster's storage class
          accessModes: ["ReadWriteMany"]
          resources:
            requests:
              storage: 100Gi
```

### Monitoring Prometheus Resources

Monitor these key metrics to track Prometheus health and adjust resources as needed:

* `prometheus_tsdb_head_series`: Current number of active series
* `process_resident_memory_bytes`: Memory usage by Prometheus
* `prometheus_engine_query_duration_seconds`: Query performance
* `rate(prometheus_tsdb_head_samples_appended_total[5m])`: Ingestion rate

Regular monitoring of these metrics will help ensure adequate resources as your monitoring needs grow.

## Long-Term Storage with Thanos

### Overview

Thanos extends Prometheus capabilities by providing:

- Long-term metrics storage using object storage (S3)
- Global query view across multiple Prometheus instances
- Downsampling for efficient long-term data storage

### Deployment Steps

#### 1. Configure Thanos Sidecar in kube-prometheus-stack

Create a values file for Thanos configuration:

```yaml
prometheus:
  prometheusSpec:
    serviceMonitorSelectorNilUsesHelmValues: false #this is used to enable all service monitors
    podMonitorSelectorNilUsesHelmValues: false
    disableCompaction: true
    retention: 2d # you can change retention period
    retentionSize: "1GB" #this is used to avoid log storage in pvc and avoid crashing
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: standard #you can change storage type depend on cloud
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 20Gi #you can change storage configuration
    thanos:
      image: quay.io/thanos/thanos:v0.28.1  #image of thanos to run as sidecar
      objectStorageConfig:
        existingSecret:
          name: thanos-objstore-config  #secret that we will create further
          key: thanos.yaml

  thanosService:
    enabled: true # this will enable a service for service discovery
    annotations: {}
    labels: {}
    externalTrafficPolicy: Cluster
    type: ClusterIP
    portName: grpc
    port: 10901
    targetPort: "grpc"
    httpPortName: http
    httpPort: 10902
    targetHttpPort: "http"
    clusterIP: ""
    nodePort: 30901
    httpNodePort: 30902
```

Apply the configuration:

```bash
helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack   --namespace prometheus   --values kube-prometheus-stack.values.yaml
```

#### 2. Configure Object Storage

Create a configuration file for the object storage:

```yaml
type: s3
config:
  bucket: thanos-store
  endpoint: s3.<region>.amazonaws.com
  region: "ap-south-1"
  access_key: <aws-access-key>
  secret_key: <aws-secret-key>
```

Create a secret with this configuration:

```bash
kubectl -n prometheus create secret generic thanos-objstore-config --from-file=thanos.yaml=thanos-storage-config.yaml
```

#### 3. Deploy Thanos Components

Create a namespace for Thanos:

```bash
kubectl create ns thanos
```

Deploy Thanos components (query, store, receive) using manifests from the official repository or a custom configuration:

```bash
git clone https://github.com/thanos-io/kube-thanos.git
#Update the manifests and then apply them
kubectl apply -f thanos/manifests/ -n thanos
```

Ensure the Thanos Query configuration includes store endpoints to connect to the Prometheus sidecar:
Make sure it matches your prometheus thanos service 

- --store=dnssrv+_grpc._tcp.prometheus-kube-prometheus-thanos-discovery.prometheus.svc.cluster.local:10901

```yaml
      - args:
        - query
        - --grpc-address=0.0.0.0:10901
        - --http-address=0.0.0.0:9090
        - --log.level=info
        - --log.format=logfmt
        - --query.replica-label=prometheus_replica
        - --query.replica-label=rule_replica
        - --endpoint=dnssrv+_grpc._tcp.thanos-store.thanos.svc.cluster.local:10901
        - --endpoint=dnssrv+_grpc._tcp.thanos-receive-ingestor-default.thanos.svc.cluster.local:10901
        - --store=dnssrv+_grpc._tcp.thanos-store.thanos.svc.cluster.local:10901
        - --store=dnssrv+_grpc._tcp.prometheus-kube-prometheus-thanos-discovery.prometheus.svc.cluster.local:10901
        - --query.auto-downsampling
```

We need to configure a secret with a name as requested in thanos-store-statefulSet.yaml.
The below values are added in manifest. You just need to cross-check the name and key to avoid any name-conflict.

```yaml
  env:
  - name: OBJSTORE_CONFIG
    valueFrom:
      secretKeyRef:
        key: thanos.yaml
        name: thanos-objectstorage
```

Create a secret for the Thanos Store component:

```bash
kubectl -n thanos create secret generic thanos-objectstorage --from-file=thanos.yaml=thanos-storage-config.yaml
```

#### 4. Verification

Verify all components are running:

```bash
kubectl get pods -n thanos
```

Access the Thanos Query UI:

```bash
kubectl port-forward svc/thanos-query -n thanos 9090:9090
```

Navigate to http://localhost:9090 and check the "Stores" page to confirm all components are connected.

### Architecture

With Thanos integration, the architecture expands to:

```
┌─────────────────────────────────────────────────────────────┐
│                  kube-prometheus-stack                      │
├───────────┬───────────┬───────────┬───────────┬─────────────┤
│ Prometheus│Alertmanager│  Grafana  │    Node   │  Kube State│
│  + Thanos │           │           │  Exporter │  Metrics    │
│  Sidecar  │           │           │           │             │
├───────────┴───────────┴───────────┴───────────┴─────────────┤
│                    Prometheus Operator                      │
└───────────────────────────┬─────────────────────────────────┘
                           │
┌───────────────────────────┴─────────────────────────────────┐
│                       Thanos Components                     │
├───────────┬───────────┬───────────┬─────────────────────────┤
│   Query   │   Store   │  Receive  │                         │
│           │           │           │                         │
└───────────┴───────────┴───────────┘                         │
                           │                                  │
                           ▼                                  │
                 ┌─────────────────────┐                      │
                 │    Object Storage   │                      │
                 │      (S3 Bucket)    │                      │
                 └─────────────────────┘                      │
└─────────────────────────────────────────────────────────────┘
```

### Benefits

- **Historical Data Access**: Query metrics beyond the local Prometheus retention period
- **Reduced Local Storage**: Maintain shorter retention in Prometheus (2 days) while keeping long-term data in object storage
- **High Availability**: Access metrics even if Prometheus instances are temporarily unavailable
- **Efficient Storage**: Downsampling reduces storage requirements for historical data
- **Global Query View**: Unified query interface across all Prometheus instances

#### c. Other Important Configurations

Consider customizing these additional settings based on your requirements:

```yaml
# Storage configurations
prometheus:
  prometheusSpec:
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: standard
          resources:
            requests:
              storage: 50Gi

# Retention settings
    retention: 15d
    retentionSize: "30GB"

# Resource allocations
    resources:
      requests:
        memory: 2Gi
        cpu: 500m
      limits:
        memory: 4Gi
        cpu: 1000m

# Grafana settings
grafana:
  adminPassword: "prom-operator"
  persistence:
    enabled: true
    storageClassName: standard
    size: 10Gi
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
      - name: 'default'
        orgId: 1
        folder: ''
        type: file
        disableDeletion: false
        editable: true
        options:
          path: /var/lib/grafana/dashboards/default
```

### 4. Deploy the Prometheus Stack

```bash
helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
  --create-namespace \
  --namespace prometheus \
  --values kube-prometheus-stack.values
```

Alternatively, using the generate-name option:

```bash
helm install prometheus-community/kube-prometheus-stack \
  --create-namespace \
  --namespace prometheus \
  --generate-name \
  --values kube-prometheus-stack.values
```

## Grafana LDAP Authentication and Dashboard Provisioning

### Overview

This section describes how to configure Grafana for LDAP authentication with role-based access control (RBAC) and automated dashboard provisioning. This setup enables:

- Authentication via enterprise LDAP directory
- Role-based access mapped to LDAP groups
- Project-based dashboard organization
- User-specific resource monitoring

### LDAP Authentication Configuration

#### 1. Create LDAP Credentials Secret

First, create a Kubernetes secret to store the LDAP credentials:

```bash
kubectl create secret generic grafana-ldap-credentials \
  --namespace prometheus \
  --from-literal=LDAP_PASSWORD='your-ldap-password'
```

#### 2. Configure LDAP in Helm Values

Add the following LDAP configuration to your `kube-prometheus-stack.values.yaml` file:

```yaml
grafana:
  ldap:
    enabled: true
    config: |
      [[servers]]
      host = "63.176.217.151"
      port = 389
      use_ssl = false
      start_tls = false
      ssl_skip_verify = true
      bind_dn = "uid=admin,cn=users,cn=accounts,dc=kaust,dc=local"
      bind_password = "${LDAP_PASSWORD}"
      # Important: Restrict users to only those in Grafana groups
      # This ensures only users in at least one of our Grafana groups can log in
      search_filter = "(&(objectClass=person)(uid=%s)(|(memberOf=cn=grafana-admins,cn=groups,cn=accounts,dc=kaust,dc=local)(memberOf=cn=grafana-members,cn=groups,cn=accounts,dc=kaust,dc=local)(memberOf=*cn=grafana-project-*,cn=groups,cn=accounts,dc=kaust,dc=local)))"
      search_base_dns = ["cn=users,cn=accounts,dc=kaust,dc=local"]
      # Map attributes
      [servers.attributes]
      name = "givenName"
      surname = "sn"
      username = "uid"
      member_of = "memberOf"
      email = "mail"
      # Admin group mapping
      [[servers.group_mappings]]
      group_dn = "cn=grafana-admins,cn=groups,cn=accounts,dc=kaust,dc=local"
      org_role = "Admin"
      org_id = 1
      # Regular users mapping
      [[servers.group_mappings]]
      group_dn = "cn=grafana-members,cn=groups,cn=accounts,dc=kaust,dc=local"
      org_role = "Viewer"
      org_id = 1
      # Project group pattern - use group filter instead of wildcard
      [servers.group_search]
      base_dn = "cn=groups,cn=accounts,dc=kaust,dc=local"
      filter = "(cn=grafana-project-*)"
      # Project user mapping
      [[servers.group_mappings]]
      group_dn = "*" # This applies to groups matched by the filter above
      org_role = "Viewer"
      org_id = 1
      # Another server block for regular project members
      [[servers]]
      host = "63.176.217.151"
      port = 389
      use_ssl = false
      bind_dn = "uid=admin,cn=users,cn=accounts,dc=kaust,dc=local"
      bind_password = "${LDAP_PASSWORD}"
      # Same search filter as above to restrict access
      search_filter = "(&(objectClass=person)(uid=%s)(|(memberOf=cn=grafana-admins,cn=groups,cn=accounts,dc=kaust,dc=local)(memberOf=*cn=grafana-admins-project-*,cn=groups,cn=accounts,dc=kaust,dc=local)))"
      search_base_dns = ["cn=users,cn=accounts,dc=kaust,dc=local"]
      [servers.attributes]
      name = "givenName"
      surname = "sn"
      username = "uid"
      member_of = "memberOf"
      email = "mail"
      # Normal project user group search
      [servers.group_search]
      base_dn = "cn=groups,cn=accounts,dc=kaust,dc=local"
      filter = "(cn=grafana-admins-project-*)"
      # Normal project user mapping
      [[servers.group_mappings]]
      group_dn = "*"
      org_role = "Editor"
      org_id = 1

  # Enhanced LDAP security settings
  grafana.ini:
    auth.ldap:
      enabled: true
      config_file: /etc/grafana/ldap.toml
      allow_sign_up: true
      verbose_logging: true
      sync_cron: "*/10 * * * *"
    auth:
      # Only allow sign-in through LDAP, disable other methods
      disable_login_form: false
      oauth_auto_login: false
      disable_signout_menu: false
    # Enable team sync
    teams:
      auto_create: true
    security:
      # Require users to be members of at least one group to log in
      strict_auth: true
    # Debug logging for troubleshooting
    log:
      level: debug
      filters: "ldap:debug"

  # Configure secrets for LDAP password
  envFromSecret: "grafana-ldap-credentials"
```

### Dashboard Provisioning

#### 1. Configure Dashboard Providers

Add the following dashboard provider configuration to enable automatic dashboard discovery:

```yaml
grafana:
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'default'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: false
          updateIntervalSeconds: 30
          allowUiUpdates: true
          options:
            path: /var/lib/grafana/dashboards
```

#### 2. User & Project Dashboards

Create ConfigMaps for each dashboard to be automatically provisioned:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-personal-dashboard
  namespace: prometheus
  labels:
    grafana_dashboard: "1"
data:
  user-personal-dashboard.json: |
    {
      "annotations": {"list": []},
      "editable": false,
      "panels": [
        {
          "title": "Personal Workload Summary",
          "type": "stat",
          "gridPos": {"h": 4, "w": 8, "x": 0, "y": 0},
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "options": {"colorMode": "value", "graphMode": "area"},
          "targets": [{"expr": "count(kube_pod_info{namespace=\"$namespace\"})"}]
        },
        {
          "title": "CPU Usage",
          "type": "gauge",
          "gridPos": {"h": 8, "w": 8, "x": 0, "y": 4},
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "options": {"showThresholdLabels": true, "showThresholdMarkers": true},
          "fieldConfig": {
            "defaults": {
              "min": 0,
              "max": 100,
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"value": null, "color": "green"},
                  {"value": 60, "color": "yellow"},
                  {"value": 80, "color": "red"}
                ]
              },
              "unit": "percent"
            }
          },
          "targets": [
            {"expr": "sum(rate(container_cpu_usage_seconds_total{namespace=\"$namespace\"}[5m])) / sum(kube_pod_container_resource_limits{namespace=\"$namespace\", resource=\"cpu\"}) * 100 or vector(0)", "legendFormat": "CPU Usage %"}
          ]
        },
        {
          "title": "Memory Usage",
          "type": "gauge",
          "gridPos": {"h": 8, "w": 8, "x": 8, "y": 4},
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "options": {"showThresholdLabels": true, "showThresholdMarkers": true},
          "targets": [
            {"expr": "sum(container_memory_usage_bytes{namespace=\"$namespace\"}) / sum(kube_pod_container_resource_limits{namespace=\"$namespace\", resource=\"memory\"}) * 100 or vector(0)", "legendFormat": "Memory Usage %"}
          ]
        },
        {
          "title": "GPU Utilization",
          "type": "gauge",
          "gridPos": {"h": 8, "w": 8, "x": 16, "y": 4},
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "options": {"showThresholdLabels": true, "showThresholdMarkers": true},
          "targets": [
            {"expr": "avg(DCGM_FI_DEV_GPU_UTIL{namespace=\"$namespace\"}) or vector(0)", "legendFormat": "GPU Utilization %"}
          ]
        },
        {
          "title": "GPU Memory Usage",
          "type": "timeseries",
          "gridPos": {"h": 8, "w": 12, "x": 0, "y": 20},
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "fieldConfig": {"defaults": {"unit": "bytes"}},
          "targets": [
            {"expr": "DCGM_FI_DEV_FB_USED{namespace=\"$namespace\"}", "legendFormat": "GPU {{gpu}} - Pod: {{pod_name}}"}
          ]
        }
      ],
      "templating": {
        "list": [
          {
            "name": "namespace",
            "type": "query",
            "label": "Namespace",
            "query": "label_values(kube_namespace_labels, namespace)",
            "refresh": 1,
            "sort": 1,
            "includeAll": false,
            "multi": false
          }
        ]
      },
      "refresh": "10s",
      "time": {"from": "now-1h", "to": "now"},
      "title": "Personal Resource Dashboard",
      "uid": "user-personal-dashboard"
    }
```

#### 3. Configure Project Admin Dashboard

Create the project admin dashboard with project-specific metrics:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: project-admin-dashboard
  namespace: prometheus
  labels:
    grafana_dashboard: "1"
data:
  project-admin-dashboard.json: |
    {
      "annotations": {"list": []},
      "editable": true,
      "panels": [
        {
          "title": "Project Overview",
          "type": "row",
          "gridPos": {"h": 1, "w": 24, "x": 0, "y": 0},
          "collapsed": false
        },
        {
          "title": "Total Project Pods",
          "type": "stat",
          "gridPos": {"h": 4, "w": 6, "x": 0, "y": 1},
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "options": {"colorMode": "value", "graphMode": "area"},
          "targets": [
            {"expr": "count(kube_pod_labels{label_project=\"$project\"})", "legendFormat": "Pods"}
          ]
        },
        {
          "title": "Total CPU Usage",
          "type": "gauge",
          "gridPos": {"h": 8, "w": 8, "x": 0, "y": 5},
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "options": {"showThresholdLabels": true, "showThresholdMarkers": true},
          "targets": [
            {"expr": "sum(rate(container_cpu_usage_seconds_total{pod=~\".+\"}[5m]) * on(pod) group_left(label_project) kube_pod_labels{label_project=\"$project\"}) / sum(kube_pod_container_resource_limits{pod=~\".+\", resource=\"cpu\"} * on(pod) group_left(label_project) kube_pod_labels{label_project=\"$project\"}) * 100 or vector(0)", "legendFormat": "CPU Usage %"}
          ]
        },
        {
          "title": "Total GPU Usage",
          "type": "gauge",
          "gridPos": {"h": 8, "w": 8, "x": 16, "y": 5},
          "datasource": {"type": "prometheus", "uid": "prometheus"},
          "options": {"showThresholdLabels": true, "showThresholdMarkers": true},
          "targets": [
            {"expr": "avg(DCGM_FI_DEV_GPU_UTIL{pod=~\".+\"} * on(pod) group_left(label_project) kube_pod_labels{label_project=\"$project\"}) or vector(0)", "legendFormat": "GPU Utilization %"}
          ]
        }
      ],
      "templating": {
        "list": [
          {
            "name": "project",
            "type": "query",
            "label": "Project",
            "query": "label_values(kube_pod_labels, label_project)",
            "refresh": 1,
            "sort": 1,
            "includeAll": false,
            "multi": false
          }
        ]
      },
      "refresh": "10s",
      "time": {"from": "now-6h", "to": "now"},
      "title": "Project Admin Dashboard",
      "uid": "project-admin-dashboard"
    }
```

#### 4. Enable Dashboard Sidecar

Configure the Grafana sidecar to automatically discover and load dashboards:

```yaml
grafana:
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      searchNamespace: prometheus
      provider:
        allowUiUpdates: true
        foldersFromFilesStructure: true
```

### Automated Folder & Dashboard Provisioning

Create a CronJob to automatically set up project-specific folders and manage permissions:

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: grafana-folder-setup
  namespace: prometheus
spec:
  schedule: "0 * * * *"  # Run hourly
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: grafana-folder-setup
            image: curlimages/curl:7.82.0
            command: ["/bin/bash", "-c"]
            args:
              - |
                #!/bin/bash

                # Install required tools
                apt-get update && apt-get install -y curl ldap-utils jq

                # Create API key and query LDAP for projects
                # Set up folders for each project
                # Configure team permissions based on LDAP groups

                echo "Folder and permission setup completed"
            env:
              - name: LDAP_PASSWORD
                valueFrom:
                  secretKeyRef:
                    name: grafana-ldap-credentials
                    key: LDAP_PASSWORD
              - name: GF_SECURITY_ADMIN_PASSWORD
                valueFrom:
                  secretKeyRef:
                    name: grafana-admin-credentials
                    key: admin-password
          restartPolicy: OnFailure
```

### Verification

After configuring LDAP authentication and dashboard provisioning:

1. Verify LDAP connection:
   
   ```bash
   kubectl exec -n prometheus deploy/kube-prometheus-stack-grafana -- ldapsearch -x -H ldap://63.176.217.151 -D "uid=admin,cn=users,cn=accounts,dc=kaust,dc=local" -w "${LDAP_PASSWORD}" -b "cn=groups,cn=accounts,dc=kaust,dc=local" "(cn=grafana-*)"
   ```

2. Check if dashboards are loaded:
   
   ```bash
   kubectl exec -n prometheus deploy/kube-prometheus-stack-grafana -- ls -la /var/lib/grafana/dashboards/
   ```

3. Access Grafana and test LDAP login:
   
   ```bash
   kubectl port-forward -n prometheus svc/kube-prometheus-stack-grafana 3000:80
   ```

### Benefits

- **Role-Based Access Control**: Users only see dashboards relevant to their role
- **Project Isolation**: Project-specific dashboards show only relevant metrics
- **Automated Provisioning**: New projects automatically get appropriate dashboards
- **GPU Metrics Integration**: Comprehensive GPU utilization metrics for AI/ML workloads
- **Enterprise Authentication**: Single sign-on via existing LDAP directory
- **Self-Service Monitoring**: Users can monitor their own workloads without admin intervention

### Architecture

With LDAP and dashboard provisioning, the monitoring architecture expands to:

```
┌─────────────────────────────────────────────────────────────┐
│                  kube-prometheus-stack                      │
├───────────┬───────────┬───────────┬───────────┬─────────────┤
│ Prometheus│Alertmanager│  Grafana  │    Node   │  Kube State│
│  + Thanos │           │  + LDAP   │  Exporter │  Metrics    │
│  Sidecar  │           │           │           │             │
├───────────┴───────────┴───────────┴───────────┴─────────────┤
│                    Prometheus Operator                      │
└───────────────────────────┬─────────────────────────────────┘
                           │
                           ▼
                 ┌─────────────────────┐
                 │    LDAP Directory   │
                 │    (Authentication) │
                 └─────────────────────┘
```

## Post-Installation Verification

### 1. Verify the Deployment

```bash
# Check all resources in the prometheus namespace
kubectl get all -n prometheus

# Verify Prometheus pods are running
kubectl get pods -n prometheus -l app=prometheus

# Verify Grafana pods are running
kubectl get pods -n prometheus -l app.kubernetes.io/name=grafana
```

### 2. Check Prometheus Configuration

```bash
# Get the Prometheus pod name
PROMETHEUS_POD=$(kubectl get pods -n prometheus -l app=prometheus -o jsonpath="{.items[0].metadata.name}")

# Check the configuration
kubectl exec -n prometheus $PROMETHEUS_POD -c prometheus -- cat /etc/prometheus/prometheus.yml
```

### 3. Access the Prometheus UI (via port-forwarding)

```bash
kubectl port-forward -n prometheus svc/kube-prometheus-stack-prometheus 9090:9090
```

Then access Prometheus at http://localhost:9090

### 4. Access the Grafana UI (via port-forwarding)

```bash
kubectl port-forward -n prometheus svc/kube-prometheus-stack-grafana 3000:80
```

Then access Grafana at http://localhost:3000 (default credentials: admin / prom-operator)

## Advanced Configurations

### 1. Custom Alerting Rules

Add custom alerting rules by configuring the `additionalPrometheusRulesMap` section:

```yaml
additionalPrometheusRulesMap:
  custom-alert-rules:
    groups:
    - name: CustomAlerts
      rules:
      - alert: HighCPUUsage
        expr: node_cpu_usage_percentage > 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: High CPU usage detected
          description: "CPU usage on {{ $labels.node }} is above 90% for more than 5 minutes."
```

### 2. Thanos Integration

For long-term storage and high availability, integrate with Thanos:

```yaml
prometheus:
  prometheusSpec:
    thanos:
      image: quay.io/thanos/thanos:v0.28.0
      objectStorageConfig:
        key: thanos.yaml
        name: thanos-objstore-config
```

### 3. External Labels

Add external labels to your Prometheus instances:

```yaml
prometheus:
  prometheusSpec:
    externalLabels:
      cluster: production
      region: us-east-1
```

## Monitoring GPU Metrics

The configuration provided includes a specific job for GPU metrics collection. To ensure it works properly:

1. Verify the GPU Operator is installed in the `gpu-operator` namespace
2. Check that the metrics endpoint is exposed as expected
3. Create a test dashboard in Grafana to visualize GPU metrics

Example Grafana query for GPU utilization:

```
100 - (avg by(instance) (nvidia_gpu_memory_free_bytes) / avg by(instance) (nvidia_gpu_memory_total_bytes)) * 100
```

## Troubleshooting

### Common Issues

1. **ServiceMonitors not being discovered**:
   
   - Verify `serviceMonitorSelectorNilUsesHelmValues` is set to `false`
   - Check ServiceMonitor labels match Prometheus selector

2. **Storage Issues**:
   
   - Verify PVC is bound: `kubectl get pvc -n prometheus`
   - Check storage class exists: `kubectl get storageclass`

3. **Resource Constraints**:
   
   - Monitor resource usage: `kubectl top pods -n prometheus`
   - Adjust resource requests/limits in values file

4. **Alertmanager Configuration**:
   
   - Check alertmanager secret: `kubectl get secret -n prometheus alertmanager-kube-prometheus-stack-alertmanager`

### Debugging Steps

1. Check Prometheus logs:
   
   ```bash
   kubectl logs -n prometheus -l app=prometheus -c prometheus
   ```

2. Check Operator logs:
   
   ```bash
   kubectl logs -n prometheus -l app.kubernetes.io/name=prometheus-operator
   ```

3. Verify CRD status:
   
   ```bash
   kubectl get prometheuses.monitoring.coreos.com -n prometheus
   kubectl get alertmanagers.monitoring.coreos.com -n prometheus
   ```

## Maintenance and Upgrades

### Updating the Prometheus Stack

```bash
# Update repositories
helm repo update

# Upgrade existing installation
helm upgrade kube-prometheus-stack prometheus-community/kube-prometheus-stack \
  --namespace prometheus \
  --values kube-prometheus-stack.values
```

### Backup and Restore

1. Backup Prometheus data directory before upgrades
2. Create persistent volume snapshots if supported by your storage class
3. Export and save important Grafana dashboards

## References

- [Prometheus Operator Documentation](https://github.com/prometheus-operator/prometheus-operator)
- [kube-prometheus-stack Helm Chart](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack)
- [Prometheus Documentation](https://prometheus.io/docs/introduction/overview/)
- [Grafana Documentation](https://grafana.com/docs/)
- [GPU Monitoring](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html)
